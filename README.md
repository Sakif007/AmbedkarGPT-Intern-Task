# AmbedkarGPT-Intern-Task
AmbedkarGPT is a Retrieval-Augmented Generation (RAG) pipeline built to answer questions over Dr. B.R. Ambedkarâ€™s writings, featuring multi-document retrieval, rigorous evaluation metrics, and comparative chunking analysis.

The project combines dense document retrieval, local LLM inference, and rigorous evaluation metrics to analyze both retrieval effectiveness and answer quality.

## Purpose of the Project
Large Language Models (LLMs) are powerful but prone to hallucination when answering factual questions.
Retrieval-Augmented Generation (RAG) mitigates this by grounding model outputs in retrieved source documents.

This project aims to:

- Build a multi-document RAG pipeline
- Evaluate retrieval quality and answer quality quantitatively
- Analyze how chunk size affects performance
- Identify failure modes in retrieval and generation
- Recommend an optimal configuration based on empirical results

## ğŸ“ Repository Structure

```
AmbedkarGPT-Intern-Task/
â”‚
â”œâ”€â”€ app.py                     # Main interactive RAG application
â”œâ”€â”€ evaluation.py              # Unified evaluation script (all metrics)
â”‚
â”œâ”€â”€ corpus/                    # Source documents (6 Ambedkar texts)
â”‚   â”œâ”€â”€ speech1.txt
â”‚   â”œâ”€â”€ speech2.txt
â”‚   â”œâ”€â”€ speech3.txt
â”‚   â”œâ”€â”€ speech4.txt
â”‚   â”œâ”€â”€ speech5.txt
â”‚   â””â”€â”€ speech6.txt
â”‚
â”œâ”€â”€ test_dataset.json          # Provided evaluation dataset (25 questions)
â”œâ”€â”€ test_results.json          # Output of evaluation runs
â”‚
â”œâ”€â”€ results_analysis.md        # Detailed analysis & recommendations
â”‚
â”œâ”€â”€ requirements.txt           # Runtime + evaluation dependencies
â”œâ”€â”€ README.md                  # Project documentation
â””â”€â”€ .gitignore
```

## âš™ï¸ Setup Instructions
### 1ï¸âƒ£ Create Virtual Environment (Recommended)
```bash
python -m venv .venv
source .venv/bin/activate

```

### 2ï¸âƒ£ Install Dependencies
```bash
pip install -r requirements.txt
```
### 3ï¸âƒ£ Install & Run Ollama
Make sure Ollama is installed and running:
```bash
ollama pull mistral
ollama serve
```

## ğŸš€ Running the Application
Start the interactive question-answering system:
```bash
python app.py
```
The system will ask for a question. 3 types of command can be done here. For closing the Q/A session, a user has to prompt 'exit'. If a user wants to rebuild the database with new information, then it has to type 'rebuild'. Besides these 2 commands, every other inputs will be treated as a question and the system will retrieve relevant chunks and generates a context-grounded answer.

## ğŸ“Š Evaluation
### Run Full Evaluation
```bash
python evaluation.py
```
This runs:
### ğŸ”¹ Retrieval Metrics
- Hit@K

- Mean Reciprocal Rank (MRR)

- Precision@K

### ğŸ”¹ Answer Quality Metrics

- Answer Relevance (embedding similarity)

- Faithfulness (context consistency)

- ROUGE-L (lexical overlap)

### ğŸ”¹ Semantic Metrics
- Semantic Cosine Similarity

- BLEU Score

### ğŸ”¹ Comparative Chunking Analysis
- Small chunks (200â€“300 chars)

- Medium chunks (500â€“600 chars)

- Large chunks (800â€“1000 chars)
